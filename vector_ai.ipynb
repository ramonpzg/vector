{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a762de-214d-499d-818f-6fcf7b266964",
   "metadata": {},
   "source": [
    "# Building NLP Products Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676a88c-ab0f-42b0-8e1f-202f13ad7df3",
   "metadata": {},
   "source": [
    "> \"You shall know a word by the company it keeps.\" ~ John R. Firth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea8b98-c64f-46ed-9316-992e5e05a24c",
   "metadata": {},
   "source": [
    "![img](https://cdn.shopify.com/s/files/1/0867/3580/products/vinyl_decal_hello_words_cloud_ig4779_1800x1800.jpg?v=1571439560)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4da3fa-0dd8-425b-b045-f38acbad5ba2",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "By the end of this tutorial you will\n",
    "1. Have a better understanding of natural language processing and some of its applications.\n",
    "2. Be able to create recommendation systems based on text similarity.\n",
    "3. Be able to conduct topic modeling on your own corpus.\n",
    "4. Understand how to put together a simple app using panel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb23778-28f2-4371-b7ad-280d05a693f8",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82419249-e9bc-4590-9687-de65a15c3cf5",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. The Data\n",
    "3. Flash NLP Intro\n",
    "4. Cleaning\n",
    "5. Recommendation System\n",
    "6. Topic Modeling\n",
    "7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea680d-0e9e-4fe7-8111-0149187ed936",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1128b3-25e9-41d9-bec3-638217ed045b",
   "metadata": {},
   "source": [
    "With have been given a random corpus of articles taken from Wikipedia and our task is to come up with two products, a recommendations systems and a set of topic that best explains the model. This will help you and anyone else who picks up this notebook, understand the Wikipedia corpus better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62539a-2c1a-4bfe-b05e-8575e13a6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk, re, spacy, umap\n",
    "import pandas as pd, numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import panel as pn\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fec6d-7fa2-41b7-9726-02f6aa53c31e",
   "metadata": {},
   "source": [
    "It is possible that you will need the following packages in order to move forward. Please copy the two lines below, paste them in a new cell and run it.\n",
    "\n",
    "```python\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250ec53-3798-4137-a6ef-926664b5adb5",
   "metadata": {},
   "source": [
    "## 2. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e1d41-e0aa-4f6a-b53d-0d698d2541fd",
   "metadata": {},
   "source": [
    "The data consist of Wikipedia articles plus some additional columns inside a JSON file. Here is the schema.\n",
    "\n",
    "| Column | Content |\n",
    "|--------|---------|\n",
    "|title |Title of article|\n",
    "|url | Url of article|\n",
    "|abstract | Abstract of article|\n",
    "|body_text | Text inside article|\n",
    "|body_html | Article inside HTML|\n",
    "\n",
    "Before we do any data cleaning, let's read in the data and explore it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126886e4-2994-440c-b966-4f6add3cc815",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_list = [] # empty list that will hold a line of data for us\n",
    "\n",
    "for line in open('data.jsonl', 'r'):\n",
    "    data_list.append(json.loads(line)) # read in line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56af3b5-bed3-4efe-a607-d76027e8f301",
   "metadata": {},
   "source": [
    "Let's see how many articles we have and then examine the very first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce91d80-1b40-4c11-b77c-b5683d03a07f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(data_list), data_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64205a36-a77c-4aad-920c-18f8fdc9a38d",
   "metadata": {},
   "source": [
    "Now that we have a nice list of dictionaries, we can create a pandas DataFrame. You can think of pandas DataFrames as as Excel spreadsheets we can use to hold and manipulate our data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3f85a-01f1-438c-8d5b-deffd0f5b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca054113-dc4d-43dc-9212-0175a577557b",
   "metadata": {},
   "source": [
    "## 3. Flash NLP Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7095fab-9cf3-4964-81af-c1c963d9ffe8",
   "metadata": {},
   "source": [
    "We can use the `.loc[index, column]` method on our dataframe, select one column and one row using a comma to separate both, and examine a prettier version of the text using the python function `pprint()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3852abe-aff2-4b39-8120-f24631113fd0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_article = df.loc[10, 'body_text']\n",
    "pprint(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fa91a-aa56-4b94-bc70-da38966b1ca0",
   "metadata": {},
   "source": [
    "Notice how the review above is quite messy and it has a lot of characters that, for all intents and purposes, will not be useful for our analysis. Let's examine a cleaner version of the article above by running it through spaCy's tokenizer. When we tokenize a document, we are separating all of its content into each of its components, i.e. words, numbers, punctiations and the like, to make it easier to process and to run computations on it.\n",
    "\n",
    "For this part, we will load an english model, instantiate it and pass an example article through it. You may need to run the cell below first to download the english model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea1447-3d50-4959-bb35-0a58f23b344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12c1f7-b1cf-45ea-844f-2503fbbaa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162592d-66d5-43e9-9233-fb2327dc86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = nlp(random_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e709ec3-89bf-426c-8b3f-cea604ebcaee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7097ec-da16-4b94-9729-7fab88717e4c",
   "metadata": {},
   "source": [
    "Notice how much nicer our article looks like now.\n",
    "\n",
    "We can also grab sentences and view them one by one we wanted to using the attribute `.sents` and the built in python function `next()`. Conversely, we can add it to a loop and show each of the sentences in an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d809afe-d1d5-47b8-8cde-3db9d4e34fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(enumerate(parsed_article.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047193a-44c4-44ba-b6b0-9d7fd7a2293c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_article.sents):\n",
    "    print(f\"Sentence #{num}:\\n {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05976609-c063-452b-a9a6-e7cae20d6f7b",
   "metadata": {},
   "source": [
    "We can also have a look at the different kinds of entities in an article. These entities can be a person (called PERSON), and number (called CARDINAL), a geopolitical entity (called GPE), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863365f-25ed-498e-a44d-bff6297357d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_article.ents):\n",
    "    print(f\"Entity #{num}: {entity} -- {entity.label_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e42071-6918-46b9-b192-ad5ac595ed51",
   "metadata": {},
   "source": [
    "We can also check weather a word is a stopword or a punctuation, or we can even lemmatize our articles. Lemmatization is a way of taking the root of a word and bringing similar words to a common denominator, for example, was will become be and most plural words will be singular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1ce74-bf7f-44af-897d-2d583867da8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we are taking out of the parsed article each token\n",
    "token_text = [token.text for token in parsed_article]\n",
    "\n",
    "# here we are lemmatizing each word possible\n",
    "token_lemmas = [token.lemma_ for token in parsed_article]\n",
    "\n",
    "# stopwords are very common so here we will extract a variable that will tell us whether\n",
    "# a word is a stopword or not\n",
    "token_stop = [token.is_stop for token in parsed_article]\n",
    "\n",
    "token_punc = [token.is_punct for token in parsed_article]\n",
    "\n",
    "# we will now add all three to a dataframe and display it without assigning it to a variable\n",
    "pd.DataFrame(zip(token_text, token_lemmas, token_punc, token_stop), columns=['Original Text', 'Lemmatized Text', 'Punctuations', 'stopwords']).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a83b2-15f5-4500-a9ec-269a777faef9",
   "metadata": {},
   "source": [
    "## 4. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6243c19-06a0-43fb-829b-a21c86546d4b",
   "metadata": {},
   "source": [
    "Let's start by checking if our dataset contains any missin values, and then evaluate the amount of memory we are currently using from our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659dbde-be10-42fb-a891-19e2324656c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f10c17-94df-490a-8095-6f6dc06a8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221dc40-b2b3-455a-8b65-498543283482",
   "metadata": {},
   "source": [
    "Over 4 GBs is a lot and it is almost certain that most of that comes from the `body_html` column. Let's get rid of it since we already have the `body_text` column, and then let's evaluate again how much data we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84251929-cbd6-46da-bb1a-e5f74f9d9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('body_html', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be9bf1-00ba-469d-968a-f67d6801f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f9174-4a81-47b4-b35b-4da754c1faf0",
   "metadata": {},
   "source": [
    "Excellent, let's deal with the titles now. It seems that every abstract starts with `Wikibooks:` so let's check if this is the case and if so, let's take that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5528ca6-e8f9-4c0a-b40c-12d2bc144166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title.str.startswith('Wikibooks: ').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969fdb2-dae5-4a91-b035-a675c8504a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_title'] = df.title.str.replace('Wikibooks: ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a80a1-c5fb-494f-b785-79e2247037ce",
   "metadata": {},
   "source": [
    "Perfect! Let's now extract the `body_text` and `abstract` columns and normalize them. This means we will the `nltk` library to,\n",
    "- tokenize the documents,\n",
    "- take out anything that is not a word or a number,\n",
    "- convert to lower case,\n",
    "- strip the spaces around the words,\n",
    "- remove stopwords (we will use spaCy's list of stopwords for this),\n",
    "- and then join the cleaned tokens back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76209dbc-ee81-4a44-a8be-606f9616fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df['body_text'].values\n",
    "abstracts = df['abstract'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89840f81-600f-4c2f-b9f4-ec7201a170dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "len(STOP_WORDS), STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f38ed6-7c4c-4906-8709-faeb7ad03956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in STOP_WORDS]\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098aa977-9fcf-4e21-83f6-771a49f43666",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize_doc(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761da5f-8359-4f6e-a0e9-5f0a331c882f",
   "metadata": {},
   "source": [
    "We will also create the same version of the function but without taking the stopwords out or converting to lowecase, to normalize the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88bf9b-c2e7-489f-a494-5466b3c3fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_abs(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.strip()\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    doc = ' '.join([token for token in tokens])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33feeb17-7125-4086-a9ab-576c8968ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_abs(df.loc[10, 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634de7e-6cc4-422a-9160-4e64fa92e962",
   "metadata": {},
   "source": [
    "Since we have about 60k articles, this operation can take quite some time unless we the cleaning process concurrently. We will do this using the `ThreadPoolExecutor()` from the `concurrent.futures` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26a0cf-3d8e-420e-a04f-b64876707011",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as e:\n",
    "    processed_articles = list(e.map(normalize_doc, articles))\n",
    "    processed_abstract = list(e.map(normalize_abs, abstracts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974d2cb-f69e-4b01-a7c9-71c5ad898482",
   "metadata": {},
   "source": [
    "We will add the cleaned versions of the documents back into the dataframe and loop over these while taking the lenght (in characters terms) of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85f826-154a-41b6-b057-80e36bc5f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['clean_text'] = processed_articles\n",
    "df['clean_abstract'] = processed_abstract\n",
    "df['len_clean_text'] = df['clean_text'].apply(len)\n",
    "df['len_dirty_text'] = df['body_text'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108baa0e-2f7a-473e-b964-52c8937edbb2",
   "metadata": {},
   "source": [
    "Let's now save our cleaned dataset in case we need to restart our notebook and begin the analysis again. We will also release a bit of memory by getting rid of all the data and variables we have loaded up since the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b045ff-4298-46f6-a8d8-73a832c89d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[['url', 'clean_abstract', 'clean_title', 'clean_text', 'len_clean_text', 'len_dirty_text']].to_parquet('clean_data/clean.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d9ab5-0795-4012-b6fd-74cfa1b59f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_list\n",
    "del df\n",
    "del articles\n",
    "del abstracts\n",
    "del processed_articles\n",
    "del processed_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f34ee9-d98c-4503-86e9-e2c2c04e07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('clean_data/clean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d022ce-fae6-453b-9429-a369cbaa5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625e86c-1171-47fa-8cdf-b3dd34095049",
   "metadata": {},
   "source": [
    "It wouldn't make any sense to feed to our algorithms articles with zero words, so let's examine the distribution of characters among both, the raw and the clean version of our articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e490d0-d0e5-4c9f-9451-67468c0a41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['len_clean_text', 'len_dirty_text']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75354bf1-c6a9-4626-b364-402f161f34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['len_clean_text', 'len_dirty_text']].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd0cfe-2be5-448c-b2e2-bc034fdd342c",
   "metadata": {},
   "source": [
    "Now that we know we have a skewed distribution of characters, let's fix that by setting up a rule. We'll evaluate an article using a tweets' maximum character count, 280 at the time of writing, and filter out all articles with less than that. Let's check how many we have first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e6ec5-9192-49d3-bd6c-fdd6d49faf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter_than_a_tweet = df['len_clean_text'] < 280\n",
    "shorter_than_a_tweet.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a1ee9-4730-49a2-bf84-abc09c288610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~shorter_than_a_tweet].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b42d68-cd04-4d68-875b-0892c174ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed032e1-7dd7-42f4-9bef-fedb108c6e4d",
   "metadata": {},
   "source": [
    "# 5. Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066912f-0d5d-404f-a7f7-c2c71232eccc",
   "metadata": {},
   "source": [
    "Recommendation systems can come in many different forms and sizes. We can create a system that takes into account the behaviour of other users, or a system that only looks at similar articles or items to make a recommendation. Both are powerful systems and could cover an entire book in their own right, which is why we will focus on the latter category, the one that makes recommendations based on similar articles.\n",
    "\n",
    "To create our recommendation system we first need to convert our articles into a numerical representation. We do this with a so-called bag of words (bow). BOWs are matrices with the documents in the rows, the terms contained in all documents along the columns, and the frequency with which each term appears in each document along the values. To create this kind of representation we can use `sklearn`'s `CountVectorizer` or `TfidfVectorizer` classes. The latter being the normalized version of the former, i.e. the frequency of a word divided by the amount of documents in which it appears.\n",
    "\n",
    "To use this classes we first instantiate them, fit the data to them so that they can learn the vocabulary of our corpus, and then we tranform the corpus into a sparse matrix. These sparse matrices hold the location of all non-zero values to make it easier to store the data and compute on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94a911-28b7-4cf9-8b6c-8cb7f878c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# if you would rather work with a sample of the dataset to see how it works, use the following one\n",
    "small_df = df.sample(5_000).copy()\n",
    "\n",
    "# otherwise, use this one\n",
    "# small_df = df\n",
    "\n",
    "small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d54ee6-fdcf-4969-b3e3-b3d0262d5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# we first instantiate our class\n",
    "tf = TfidfVectorizer(min_df=0.035, max_df=0.80)\n",
    "\n",
    "# we can fit and transform the data in the same step\n",
    "tfidf_matrix = tf.fit_transform(small_df['clean_text'].values)\n",
    "\n",
    "# evaluate the shape of our matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea31046-47df-4481-9d4d-1f56d3e0f0c6",
   "metadata": {},
   "source": [
    "We can access our vocabulary with `.get_feature_names()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885797ad-19c7-42dc-8b9f-9803734a048b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db5f6c-a285-4196-946d-4166a992935a",
   "metadata": {},
   "source": [
    "The next step is to get the distance between documents and words to see how close and how far, based on words only, are two documents from one another. The `cosine_similarity` similarity function we imported earlier can do this for us, and afterwards, we can create a dataframe to evaluate our results.\n",
    "\n",
    "**Note:** this operation can take a few minutes if you are using the entire dataset. Grab some ☕️ 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa290bd-9d45-4491-8966-20575d3757bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea34bf7-6402-404d-ac73-c5a326729163",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd41dc0-553e-4f02-a146-483c60471987",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462bf8f-4d1f-4439-abe8-abf27cce9889",
   "metadata": {},
   "source": [
    "The reason we see a 5000x5000 matrix is because both halfs alonside the diagonal like are completely the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd49a4b-7638-4885-ae89-9169441323f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles_list = small_df['clean_title'].values\n",
    "abstract_list = small_df['clean_abstract'].values\n",
    "articles_list.shape, articles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102dcad7-8320-4095-8d76-7ddbb41beca7",
   "metadata": {},
   "source": [
    "Let's now\n",
    "1. pick a title at random\n",
    "2. get the index of such title\n",
    "3. select the corresponding row for such title in our new document similarity dataframe\n",
    "4. sort the index of such values\n",
    "5. return the top 5 article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fe8ff-f7a3-4338-9118-fc8ac82b6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1299e35-d190-415c-946b-e0e1699e89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_title = choice(articles_list)\n",
    "a_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc7f39-1b6f-4b8d-8fc3-2734c0714c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_idx = np.where(articles_list == a_title)[0][0]\n",
    "article_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df90a3f-62e1-4010-accc-0e98852408a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_similarities = doc_sim_df.iloc[article_idx].values\n",
    "article_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e40706-7719-422d-97c0-4c0fc7dcdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we don't select the first one as this should always be one\n",
    "similar_articles_idxs = np.argsort(-article_similarities)[1:10]\n",
    "similar_articles_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053cbfc0-c339-4322-978b-735d57fee83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_articles = articles_list[similar_articles_idxs]\n",
    "pprint(similar_articles.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d93e2-76e6-4f1b-8ab5-6274078f01ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "similar_abstracts = abstract_list[similar_articles_idxs]\n",
    "pprint(similar_abstracts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9dcd6-95af-4051-9db8-4a79f26aaa5c",
   "metadata": {},
   "source": [
    "Lastly, we will create create a mini-dashboard containing,\n",
    "1. a widget with all of our titles,\n",
    "2. a function with the steps we followed above,\n",
    "3. a panel object to store a title, the widget, and the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e47397-6216-4af5-99a0-fb0b443316d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = small_df.clean_title.unique().tolist()\n",
    "title_widget = pn.widgets.Select(value=choice(titles), options=titles, name='Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6e615-8e72-4db1-bb2d-afe073892e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(title_widget.param.value)\n",
    "def article_recommender(title_widget):\n",
    "    \n",
    "    article_idx = np.where(articles_list == title_widget)[0][0]\n",
    "    article_similarities = doc_sim_df.iloc[article_idx].values\n",
    "    similar_title_idxs = np.argsort(-article_similarities)[1:6]\n",
    "    similar_titles = articles_list[similar_title_idxs]\n",
    "    \n",
    "    return pn.Column(*similar_titles, width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0f5de-81aa-4bc0-9a3e-bdb5d5be2ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pn.pane.Markdown(f\"# Small Recommendation Engine\", style={\"color\": \"#000000\"}, width=600, height=50,\n",
    "                        sizing_mode=\"stretch_width\", margin=(10,10,10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7271ab-3737-4de0-adb9-1d4ef28f2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(text, title_widget, article_recommender, align='center', width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219da607-d0d5-418e-8949-7b3f9bc80434",
   "metadata": {},
   "source": [
    "## 6. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a47c63-7b78-4b03-b121-3f5f4bcea625",
   "metadata": {},
   "source": [
    "What is topic modeling?\n",
    "\n",
    "> \"In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both.\" ~ [Wikipedia](https://en.wikipedia.org/wiki/Topic_model)\n",
    "\n",
    "As with the recommendation engine, topic modeling requires a bag of words for the representation of the data and, in contrast, it requires a topic number as the key parameter for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69185490-a428-466f-891c-ce727177db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(strip_accents = 'unicode', min_df=0.035, max_df=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd406e8f-c081-42c4-af50-b9b100b617d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = vectorizer.fit_transform(small_df['clean_text'].values)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0404aa-e658-4020-8e83-11b08b170786",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d2085-aa22-4f49-830b-08000d7d944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=topics, # number of topics\n",
    "                                      max_iter=100, # these are the amount of times the algorithm will run\n",
    "                                      learning_method='online', \n",
    "                                      random_state=42, # setting a seed for reproducible results\n",
    "                                      n_jobs=-1) # this parameter makes sure we use all of the cores in our machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e0251-5018-4fba-8126-d6b1cc94957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lda_model.fit(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b1adc-ffb8-429d-811e-9d758bbc3c3a",
   "metadata": {},
   "source": [
    "We will create a function to explore the topics and their words to see if we can tease apart the main idea of a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff0381-0b2b-47eb-8487-828ec24f0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer, lda_model, n_words=15):\n",
    "    \"\"\"\n",
    "    This function takes our vectorizer, our model, and a\n",
    "    number of words to display the topics from our model.\n",
    "    \"\"\"\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd8953-2c41-4c31-a3c2-35f7279b74aa",
   "metadata": {},
   "source": [
    "Play around with the topic number and the words evaluated to see which amounts makes most sense to you./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d14904-3a53-455d-a90d-8456d3bbf04f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a256d-97a1-4be8-a6a5-c637dd76eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = sorted(tf.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9855f3-4b17-4b31-9826-29101cbda2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_docs = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
    "bow_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6e6e9-934b-4425-a4f6-c93c90c8015f",
   "metadata": {
    "tags": []
   },
   "source": [
    "The components of our model can be found `lda_model.components_` and can help us create different sets of dataframes, namely, terms-to-topics and document-to-topics. The former has as its values the number of times a word is assigned in a topic, and the latter is the probabily of the words in a document being contained in a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002d665-82ac-4b39-9ab0-521533882958",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term = pd.DataFrame(lda_model.components_.T, index=terms, columns=['topic_' + str(i) for i in range(topics)])\n",
    "topic_term.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013f9c3-1159-4666-a8b6-e82c7833ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame(lda_model.transform(tfidf_matrix), index=small_df.clean_title, columns=['topic_' + str(i) for i in range(topics)])\n",
    "doc_topic.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6222b7-51e1-486a-b9ee-4c8e2ba2cdf0",
   "metadata": {},
   "source": [
    "Lastly, a good way to examine the output of an LDA model is by visulizing it with nice graphs and for this we have, `pyLDAvis`. Which is a python library for visualizing topic modeling. We first load it with it's sklearn backend while enabling the notebook setting. Next we use `pyLDAvis.sklearn.prepare` and pass in our model, the bag of words, and the fitted vectorizer to get a nice interactive visualization tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41263f00-5618-4274-8f11-b2dac6da5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc9b7e-af2a-4530-9dc3-af08ed9b1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_model, bow, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ebe2b-7cf9-4f9d-a085-2ed6d94c3f28",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c11d17-90cc-4032-a00d-7e45b2c8bd23",
   "metadata": {},
   "source": [
    "Blind Spots\n",
    "\n",
    "With additional time we could have,\n",
    "1. Further tweak the parameters of the vectorizers and models;\n",
    "2. Create visualizations of both, the best topics and the document similarity to find more interesting patters;\n",
    "3. Take the title of an article out of the body of the article to create a better, less bias representation of the words within a document.\n",
    "\n",
    "Takeaways,\n",
    "1. Recommendation systems and topic modeling are both unsupervised methods;\n",
    "2. Recommendation systems can be created with or without users behavioural data;\n",
    "3. Topic modeling compresses the data into the most important and meaninful words set by you;\n",
    "4. Creating bags of words requires careful attention to the parameters;\n",
    "5. Where possible, showcase a model or system in a mini-dashboard or data visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
